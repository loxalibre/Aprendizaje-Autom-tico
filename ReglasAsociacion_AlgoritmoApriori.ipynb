{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "ReglasAsociacion_AlgoritmoApriori",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loxalibre/Aprendizaje-Autom-tico/blob/main/ReglasAsociacion_AlgoritmoApriori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMUeGMhYVCOz"
      },
      "source": [
        "<p><img alt=\"Universidad Nacional de Loja logo\" height=\"140px\" src=\"https://pbs.twimg.com/profile_images/1049739254631452673/EeXZTWRj_400x400.jpg\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1> Universidad Nacional de Loja</h1>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Carrera de Ingeniería en Sistemas (mayo-septiembre 2021)\n",
        "\n",
        "---\n",
        "Inteligencia Artificial\n",
        "\n",
        "---\n",
        "\n",
        "Estudiante: Maria Encalada\n",
        "\n",
        "---\n",
        "\n",
        "Créditos: Tomado de: https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Class21_Basic_Data_Mining_Using_Python.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOdgef9zOwYV"
      },
      "source": [
        "# Minería de reglas de asociación con el algoritmo Apriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ2ME5rrhD7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc040645-d550-4932-e82c-66c4e82cacc2"
      },
      "source": [
        "!pip install apyori  #Esto instala el paquete Apyori para utilizar el algoritmo Apriori de minería de asociación\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from apyori import apriori"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apyori in /usr/local/lib/python3.7/dist-packages (1.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQGhjEtjKB8X"
      },
      "source": [
        "filepath = \"https://raw.githubusercontent.com/loxalibre/Aprendizaje-Autom-tico/main/store_data.csv\"\n",
        "store_data = pd.read_csv(filepath, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7IcIb_ghpNZ"
      },
      "source": [
        "Llamar a la función head() para ver el aspecto del conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un923mavhsL5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "1d98b238-d004-4a33-bca7-4a2751317c63"
      },
      "source": [
        "store_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>shrimp</td>\n",
              "      <td>almonds</td>\n",
              "      <td>avocado</td>\n",
              "      <td>vegetables mix</td>\n",
              "      <td>green grapes</td>\n",
              "      <td>whole weat flour</td>\n",
              "      <td>yams</td>\n",
              "      <td>cottage cheese</td>\n",
              "      <td>energy drink</td>\n",
              "      <td>tomato juice</td>\n",
              "      <td>low fat yogurt</td>\n",
              "      <td>green tea</td>\n",
              "      <td>honey</td>\n",
              "      <td>salad</td>\n",
              "      <td>mineral water</td>\n",
              "      <td>salmon</td>\n",
              "      <td>antioxydant juice</td>\n",
              "      <td>frozen smoothie</td>\n",
              "      <td>spinach</td>\n",
              "      <td>olive oil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>burgers</td>\n",
              "      <td>meatballs</td>\n",
              "      <td>eggs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chutney</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>turkey</td>\n",
              "      <td>avocado</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mineral water</td>\n",
              "      <td>milk</td>\n",
              "      <td>energy bar</td>\n",
              "      <td>whole wheat rice</td>\n",
              "      <td>green tea</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0          1           2   ...               17       18         19\n",
              "0         shrimp    almonds     avocado  ...  frozen smoothie  spinach  olive oil\n",
              "1        burgers  meatballs        eggs  ...              NaN      NaN        NaN\n",
              "2        chutney        NaN         NaN  ...              NaN      NaN        NaN\n",
              "3         turkey    avocado         NaN  ...              NaN      NaN        NaN\n",
              "4  mineral water       milk  energy bar  ...              NaN      NaN        NaN\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANMDy-8uh2UF"
      },
      "source": [
        "Si se observan cuidadosamente los datos, podemos ver que la cabecera es en realidad la primera transacción. Cada fila corresponde a una transacción y cada columna corresponde a un artículo comprado en esa transacción específica. El NaN nos indica que el artículo representado por la columna no se compró en esa transacción específica.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js_n2gR-iDKh"
      },
      "source": [
        "## Procesamiento de datos\n",
        "\n",
        "La librería Apriori que vamos a utilizar requiere que nuestro conjunto de datos tenga la forma de una lista de listas, donde todo el conjunto de datos es una lista grande y cada operación en el conjunto de datos es una lista interior dentro de la lista grande exterior. Actualmente tiene los datos en forma de dataframe de pandas. Para convertir el dataframe de pandas en una lista de listas, ejecute el siguiente script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwJwtADyiHkM"
      },
      "source": [
        "records = []\n",
        "for i in range(0, 7501):\n",
        "    records.append([str(store_data.values[i,j]) for j in range(0, 20)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBr6pd8JiNtV"
      },
      "source": [
        "## Aplicación de Apriori\n",
        "\n",
        "El siguiente paso es aplicar el algoritmo Apriori sobre el conjunto de datos. Para ello, podemos utilizar la clase apriori que hemos importado de la biblioteca apyori.\n",
        "\n",
        "La clase apriori requiere algunos valores de parámetros para funcionar. El primer parámetro es la lista de la que se quieren extraer las reglas. El segundo parámetro es el parámetro min_support. Este parámetro se utiliza para seleccionar los elementos con valores de soporte mayores que el valor especificado por el parámetro. A continuación, el parámetro min_confidence filtra las reglas que tienen una confianza mayor que el umbral de confianza especificado por el parámetro. Del mismo modo, el parámetro min_lift especifica el valor mínimo de elevación para las reglas de la lista corta. Por último, el parámetro min_length especifica el número mínimo de elementos que se desea incluir en las reglas.\n",
        "\n",
        "Supongamos que queremos reglas sólo para los artículos que se compran al menos 5 veces al día, o 7 x 5 = 35 veces en una semana, ya que nuestro conjunto de datos corresponde a un período de una semana. El apoyo a estos artículos puede calcularse como 35/7500 = 0,0045. La confianza mínima para las reglas es del 20% o 0,2. Del mismo modo, especificamos que el valor de la elevación es 3 y, por último, la longitud mínima es 2, ya que queremos que haya al menos dos productos en nuestras reglas. Estos valores son en su mayoría elegidos arbitrariamente, por lo que puede jugar con estos valores y ver la diferencia que hace en las reglas que se obtiene de nuevo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkDTGvlniSIp"
      },
      "source": [
        "association_rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)\n",
        "association_results = list(association_rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1MTqGwMinb4"
      },
      "source": [
        "En la segunda línea aquí convertimos las reglas encontradas por la clase apriori en una lista ya que es más fácil ver los resultados de esta forma.\n",
        "\n",
        "## Visualización de los resultados\n",
        "\n",
        "Primero encontremos el número total de reglas extraídas por la clase apriori. Ejecute el siguiente script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laiaMEDNioOO"
      },
      "source": [
        "print(len(association_results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2e-JmmJi9G-"
      },
      "source": [
        "El script anterior debería devolver 48. Cada elemento corresponde a una regla.\n",
        "\n",
        "Vamos a imprimir el primer elemento de la lista association_rules para ver la primera regla. Ejecute el siguiente script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgUQhk3bjAb4"
      },
      "source": [
        "print(association_results[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdGdmSExjI5k"
      },
      "source": [
        "El primer elemento de la lista es una lista en sí misma que contiene tres elementos. El primer elemento de la lista muestra los artículos de alimentación de la norma.\n",
        "\n",
        "Por ejemplo, en el primer elemento podemos ver que la nata líquida y el pollo suelen comprarse juntos. Esto tiene sentido, ya que las personas que compran nata líquida tienen cuidado con lo que comen, por lo que es más probable que compren pollo, es decir, carne blanca, en lugar de carne roja, es decir, ternera. También podría significar que la nata ligera se utiliza habitualmente en las recetas de pollo.\n",
        "\n",
        "El valor de apoyo de la primera regla es 0,0045. Este número se calcula dividiendo el número de transacciones que contienen nata ligera entre el número total de transacciones. El nivel de confianza de la regla es de 0,2905, lo que indica que de todas las transacciones que contienen nata líquida, el 29,05% de las transacciones también contienen pollo. Por último, la elevación de 4,84 nos indica que es 4,84 veces más probable que los clientes que compran crema ligera compren pollo en comparación con la probabilidad por defecto de la venta de pollo.\n",
        "\n",
        "El siguiente script muestra la regla, el apoyo, la confianza y la elevación de cada regla de forma más clara:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56UPdDoOjMfM"
      },
      "source": [
        "for item in association_results:\n",
        "\n",
        "    # first index of the inner list\n",
        "    # Contains base item and add item\n",
        "    pair = item[0] \n",
        "    items = [x for x in pair]\n",
        "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
        "\n",
        "    #second index of the inner list\n",
        "    print(\"Support: \" + str(item[1]))\n",
        "\n",
        "    #third index of the list located at 0th\n",
        "    #of the third index of the inner list\n",
        "\n",
        "    print(\"Confidence: \" + str(item[2][0][2]))\n",
        "    print(\"Lift: \" + str(item[2][0][3]))\n",
        "    print(\"=====================================\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYIJQqLajaac"
      },
      "source": [
        "La segunda regla establece que la salsa de crema de champiñones y el escalope se compran con frecuencia. El apoyo para la salsa de crema de champiñones es de 0,0057. La confianza para esta regla es de 0,3006, lo que significa que de todas las transacciones que contienen champiñones, es probable que el 30,06% de las transacciones también contengan escalope. Por último, la elevación de 3,79 muestra que es 3,79 más probable que el escalope sea comprado por los clientes que compran salsa de crema de champiñones, en comparación con su venta por defecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_chLUblgOqj-"
      },
      "source": [
        "# kNN - k-Nearest Neighbors (vecinos más cercanos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SCTpdiFb2pL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP8qHFfLcITm"
      },
      "source": [
        "## Leer el conjunto de datos\n",
        "\n",
        "Utilizaremos el método pandas .read_csv() para leer el conjunto de datos (knnDataset.csv de iCollege). Luego utilizaremos el método .head() para observar las primeras filas de los datos, para entender mejor la información. En nuestro caso, las cabeceras de las características (columnas) nos dicen bastante poco. Esto está bien porque simplemente estamos tratando de obtener información a través de la clasificación de nuevos puntos de datos haciendo referencia a sus elementos vecinos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISvB-Gl_cV7C"
      },
      "source": [
        "# Use pandas .read_csv() method to read in classified dataset\n",
        "# index_col -> argument assigns the index to a particular column\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/loxalibre/Aprendizaje-Autom-tico/main/knnDataset.csv', index_col=0)\n",
        "# Use the .head() method to display the first few rows\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocCPiIcmcvvS"
      },
      "source": [
        "## Estandarizar (normalizar) la escala de datos para preparar el algoritmo KNN\n",
        "\n",
        "Como la distancia entre pares de puntos juega un papel crítico en la clasificación, es necesario normalizar los datos para minimizarla. Esto generará una matriz de valores. De nuevo, el KNN depende de la distancia entre cada característica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_02DtTidA0N"
      },
      "source": [
        "# Import module to standardize the scale\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Create instance (i.e. object) of the standard scaler\n",
        "scaler = StandardScaler()\n",
        "# Fit the object to all the data except the Target Class\n",
        "# use the .drop() method to gather all features except Target Class\n",
        "# axis -> argument refers to columns; a 0 would represent rows\n",
        "scaler.fit(df.drop('TARGET CLASS', axis=1))\n",
        "\n",
        "# Use scaler object to conduct a transforms\n",
        "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))\n",
        "# Review the array of values generated from the scaled features process\n",
        "scaled_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6syPA32dMLm"
      },
      "source": [
        "Aquí tenemos el conjunto de datos normalizado, menos la columna de destino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbidsXXidRa2"
      },
      "source": [
        "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n",
        "df_feat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_y3ZmyFdmOf"
      },
      "source": [
        "## Dividir los datos normalizados en conjuntos de entrenamiento y de prueba\n",
        "\n",
        "Este paso es necesario para prepararnos para el ajuste (es decir, el entrenamiento) del modelo más adelante. La variable \"X\" es una colección de todas las características. La variable \"y\" es la etiqueta objetivo que especifica la clasificación de 1 o 0. Nuestro objetivo será identificar en qué categoría debe entrar el nuevo punto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-8MgjwWdzzo"
      },
      "source": [
        "# Import module to split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Set the X and ys\n",
        "X = df_feat\n",
        "y = df['TARGET CLASS']\n",
        "# Utilizar el método train_test_split() para dividir los datos en los respectivos conjuntos\n",
        "# test_size -> el argumento se refiere al tamaño del subconjunto de prueba\n",
        "# random_state -> argumento que garantiza que la salida de la Ejecución \n",
        "# 1 será igual a la salida de la Ejecución 2, es decir, su división será siempre la misma\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4k5eEIQd4tu"
      },
      "source": [
        "Esto permite utilizar para entrenar nuestro modelo en el conjunto de entrenamiento y evaluar el modelo construido contra el conjunto de prueba para identificar los errores.\n",
        "\n",
        "## Crear y entrenar el modelo\n",
        "\n",
        "Aquí creamos un objeto KNN y utilizamos el método .fit() para entrenar el modelo. Al finalizar el modelo deberíamos recibir la confirmación de que el entrenamiento se ha completado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1DUP5LDeCT6"
      },
      "source": [
        "# Import module for KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Create KNN instance\n",
        "# n_neighbors -> argument identifies the amount of neighbors used to ID classification\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "# Fit (i.e. traing) the model\n",
        "knn.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UisOhFsieKsC"
      },
      "source": [
        "## Hacer predicciones\n",
        "\n",
        "Aquí revisamos dónde nuestro modelo fue preciso y dónde clasificó mal los elementos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD_9M8PHeQbs"
      },
      "source": [
        "# Use the .predict() method to make predictions from the X_test subset\n",
        "pred = knn.predict(X_test)\n",
        "# Review the predictions\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQXzebIeTum"
      },
      "source": [
        "## Evaluar las predicciones\n",
        "\n",
        "Evalúe el modelo revisando el informe de clasificación o la matriz de confusión. Al revisar estas tablas, podemos evaluar la precisión de nuestro modelo con nuevos valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UhpQ36XeY12"
      },
      "source": [
        "# Import classification report and confusion matrix to evaluate predictions\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Print out classification report and confusion matrix\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnr4husUegiN"
      },
      "source": [
        "## Matriz de confusión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INL9xO5Kehzk"
      },
      "source": [
        "# Print out confusion matrix\n",
        "cmat = confusion_matrix(y_test, pred)\n",
        "#print(cmat)\n",
        "print('TP - True Negative {}'.format(cmat[0,0]))\n",
        "print('FP - False Positive {}'.format(cmat[0,1]))\n",
        "print('FN - False Negative {}'.format(cmat[1,0]))\n",
        "print('TP - True Positive {}'.format(cmat[1,1]))\n",
        "print('Accuracy Rate: {}'.format(np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))))\n",
        "print('Misclassification Rate: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8dfjqdlex9P"
      },
      "source": [
        "## Evaluar valores K alternativos para obtener mejores predicciones\n",
        "\n",
        "Para simplificar el proceso de evaluación de múltiples casos de valores k, creamos una función para derivar el error utilizando la media cuando nuestras predicciones no eran iguales a los valores de prueba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5-qqmce2u_"
      },
      "source": [
        "# Generate function to add error rates of KNN with various k-values\n",
        "# error_rate -> empty list to gather error rates at various k-values\n",
        "# for loop -> loops through k values 1 to 39\n",
        "# knn -> creates instance of KNeighborsClassifier with various k\n",
        "# knn.fit -> trains the model\n",
        "# pred_i -> conducts predictions from model on test subset\n",
        "# error_rate.append -> adds error rate of model with various k-value, using the average where prediction not\n",
        "# equal to the test values\n",
        "error_rate = []\n",
        "for i in range(1,40):\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error_rate.append(np.mean(pred_i != y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jOKuu_ye58T"
      },
      "source": [
        "## Taza de error de trazo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qXtL52je8uD"
      },
      "source": [
        "# Configure and plot error rate over k values\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1,40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
        "plt.title('Error Rate vs. K-Values')\n",
        "plt.xlabel('K-Values')\n",
        "plt.ylabel('Error Rate')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csypu5V-fE8j"
      },
      "source": [
        "Aquí vemos que la tasa de error sigue disminuyendo a medida que aumentamos el valor k. Una imagen dice más que mil palabras. O al menos aquí podemos entender qué valor de k conduce a un modelo óptimo. El valor de k de 15 parece dar una tasa de error decente sin demasiado ruido, como vemos con valores de k de 25 y mayores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpW_kaOhfMgw"
      },
      "source": [
        "## Ajustar el valor de K por las evaluaciones de la tasa de error\n",
        "\n",
        "Esto es sólo un ajuste fino de nuestro modelo para aumentar la precisión. Tendremos que volver a entrenar nuestro modelo con el nuevo valor k."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3vI_Tc2fRy3"
      },
      "source": [
        "# Retrain model using optimal k-value\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "knn.fit(X_train, y_train)\n",
        "pred = knn.predict(X_test)\n",
        "# Print out classification report and confusion matrix\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzj7Z_HWfYnP"
      },
      "source": [
        "# Print out confusion matrix\n",
        "cmat = confusion_matrix(y_test, pred)\n",
        "#print(cmat)\n",
        "print('TP - True Negative {}'.format(cmat[0,0]))\n",
        "print('FP - False Positive {}'.format(cmat[0,1]))\n",
        "print('FN - False Negative {}'.format(cmat[1,0]))\n",
        "print('TP - True Positive {}'.format(cmat[1,1]))\n",
        "print('Accuracy Rate: {}'.format(np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))))\n",
        "print('Misclassification Rate: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JqJg3QjJcAF"
      },
      "source": [
        "# Máquinas de vectores de apoyo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65iJxmHeJcAG"
      },
      "source": [
        "Las máquinas de vectores de apoyo (SVM) son una clase particularmente potente y flexible de algoritmos supervisados tanto para la clasificación como para la regresión.\n",
        "\n",
        "Comenzando con las importaciones estándar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkJpyh1hJcAH"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC1vil7nJcAM"
      },
      "source": [
        "## Motivación de las máquinas de vectores de apoyo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcXXyGR0JcAO"
      },
      "source": [
        "En nuestra discusión sobre la clasificación bayesiana, aprendimos un modelo simple que describe la distribución de cada clase subyacente, y utilizamos estos modelos generativos para determinar probabilísticamente las etiquetas de los nuevos puntos.\n",
        "Ese fue un ejemplo de *clasificación generativa*; aquí consideraremos en cambio la *clasificación discriminativa*: en lugar de modelar cada clase, simplemente encontramos una línea o curva (en dos dimensiones) o colector (en múltiples dimensiones) que divide las clases entre sí.\n",
        "\n",
        "Como ejemplo de esto, consideremos el caso simple de una tarea de clasificación, en la que las dos clases de puntos están bien separadas:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXPqHsDqJcAR"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F5-u-elJcAW"
      },
      "source": [
        "Un clasificador lineal discriminativo intentaría trazar una línea recta que separara los dos conjuntos de datos, y así crear un modelo de clasificación.\n",
        "Para datos bidimensionales como los mostrados aquí, esta es una tarea que podríamos hacer a mano.\n",
        "Pero inmediatamente vemos un problema: ¡hay más de una línea divisoria posible que puede discriminar perfectamente entre las dos clases!\n",
        "\n",
        "Podemos dibujarlas de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuBAXzJKJcAY"
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qErgyr-VJcAb"
      },
      "source": [
        "Se trata de tres separadores *muy* diferentes que, sin embargo, discriminan perfectamente estas muestras.\n",
        "Dependiendo de cuál elijamos, un nuevo punto de datos (por ejemplo, el marcado por la \"X\" en este gráfico) tendrá una etiqueta diferente.\n",
        "Evidentemente, nuestra simple intuición de \"trazar una línea entre clases\" no es suficiente, y tenemos que pensar un poco más profundamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqi8lCy6JcAc"
      },
      "source": [
        "## Support Vector Machines: Maximizando el *Margen*\n",
        "\n",
        "Las máquinas de vectores de apoyo ofrecen una forma de mejorar esto.\n",
        "La intuición es la siguiente: en lugar de simplemente dibujar una línea de ancho cero entre las clases, podemos dibujar alrededor de cada línea un *margen* de cierta anchura, hasta el punto más cercano.\n",
        "He aquí un ejemplo de cómo podría ser esto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bz594DIJcAd"
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaH85hgyJcAh"
      },
      "source": [
        "En las máquinas de vectores de apoyo, la línea que maximiza este margen es la que elegiremos como modelo óptimo.\n",
        "Las máquinas de vectores de apoyo son un ejemplo de este tipo de estimador de *máximo margen*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz6LRFxlJcAi"
      },
      "source": [
        "### Ajuste de una máquina de vectores de soporte\n",
        "\n",
        "Veamos el resultado de un ajuste real a estos datos: utilizaremos el clasificador de vectores de soporte de Scikit-Learn para entrenar un modelo SVM en estos datos.\n",
        "Por el momento, utilizaremos un kernel lineal y estableceremos el parámetro ``C`` a un número muy grande (discutiremos el significado de estos en más profundidad momentáneamente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCOc-k_xJcAj"
      },
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdTJTmg4JcAn"
      },
      "source": [
        "Para visualizar mejor lo que está sucediendo aquí, vamos a crear una función de conveniencia rápida que trazará los límites de decisión SVM para nosotros:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJMC3rP_JcAo"
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B83_FoyJcAr"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADxHnv-JcAu"
      },
      "source": [
        "Esta es la línea divisoria que maximiza el margen entre los dos conjuntos de puntos.\n",
        "Fíjate en que algunos de los puntos de entrenamiento apenas tocan el margen: están indicados por los círculos negros en esta figura.\n",
        "Estos puntos son los elementos pivotantes de este ajuste, y se conocen como los *vectores de soporte*, y dan nombre al algoritmo.\n",
        "En Scikit-Learn, la identidad de estos puntos se almacena en el atributo ``support_vectors_`` del clasificador:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aimz6xKsJcAv"
      },
      "source": [
        "model.support_vectors_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rko1qBxlJcAy"
      },
      "source": [
        "Una de las claves del éxito de este clasificador es que, para el ajuste, sólo importa la posición de los vectores de soporte; ¡los puntos más alejados del margen que estén en el lado correcto no modifican el ajuste!\n",
        "Técnicamente, esto se debe a que estos puntos no contribuyen a la función de pérdida utilizada para ajustar el modelo, por lo que su posición y número no importan mientras no crucen el margen.\n",
        "\n",
        "Podemos ver esto, por ejemplo, si trazamos el modelo aprendido a partir de los primeros 60 puntos y los primeros 120 puntos de este conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrbDVH_dJcAz"
      },
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjp_8u_vJcA3"
      },
      "source": [
        "En el panel de la izquierda, vemos el modelo y los vectores de apoyo para 60 puntos de entrenamiento.\n",
        "En el panel de la derecha, hemos duplicado el número de puntos de entrenamiento, pero el modelo no ha cambiado: los tres vectores de apoyo del panel de la izquierda siguen siendo los vectores de apoyo del panel de la derecha.\n",
        "Esta insensibilidad al comportamiento exacto de los puntos distantes es uno de los puntos fuertes del modelo SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8vdKialJcA_"
      },
      "source": [
        "### Más allá de los límites lineales: Kernel SVM\n",
        "\n",
        "Donde la SVM se vuelve extremadamente potente es cuando se combina con *kernels*.\n",
        "\n",
        "Para motivar la necesidad de los núcleos, veamos algunos datos que no son linealmente separables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hmL_1zDJcBA"
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSRz1I7WJcBD"
      },
      "source": [
        "Está claro que ninguna discriminación lineal podrá *nunca* separar estos datos. Así que ahora tenemos que pensar en cómo podríamos proyectar los datos en una dimensión más alta, de manera que un separador lineal *sería* suficiente.\n",
        "\n",
        "Por ejemplo, una proyección sencilla que podríamos utilizar sería calcular una *función de base radial* centrada en el grupo central:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x0syBMiJcBE"
      },
      "source": [
        "r = np.exp(-(X ** 2).sum(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkmztFsxJcBH"
      },
      "source": [
        "Podemos visualizar esta dimensión extra de los datos mediante una tridimensionalidad:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_YYUQ1pJcBI"
      },
      "source": [
        "ax = plt.subplot(projection='3d')\n",
        "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46oIOcaHJcBK"
      },
      "source": [
        "Podemos ver que con esta dimensión adicional, los datos se vuelven trivialmente separables linealmente, dibujando un plano de separación en, digamos, *r*=0,7.\n",
        "\n",
        "Aquí tuvimos que elegir y afinar cuidadosamente nuestra proyección: si no hubiéramos centrado nuestra función de base radial en el lugar correcto, no habríamos visto resultados tan limpios y linealmente separables.\n",
        "En general, la necesidad de hacer tal elección es un problema: nos gustaría encontrar de alguna manera automática las mejores funciones de base a utilizar.\n",
        "\n",
        "Una estrategia para este fin es calcular una función de base centrada en *todos* los puntos del conjunto de datos, y dejar que el algoritmo SVM tamice los resultados.\n",
        "Este tipo de transformación de la función base se conoce como *transformación del núcleo*, ya que se basa en una relación de similitud (o núcleo) entre cada par de puntos.\n",
        "\n",
        "Un problema potencial de esta estrategia -proyectar $N$ puntos en $N$ dimensiones- es que podría ser muy intensiva en términos de computación a medida que $N$ crece.\n",
        "Sin embargo, gracias a un pequeño procedimiento conocido como el [*truco del núcleo*] (https://en.wikipedia.org/wiki/Kernel_trick), se puede realizar un ajuste en los datos transformados por el núcleo de forma implícita, es decir, ¡sin construir nunca la representación completa de $N$ dimensiones de la proyección del núcleo!\n",
        "Este truco del kernel está incorporado en la SVM, y es una de las razones por las que el método es tan poderoso.\n",
        "\n",
        "En Scikit-Learn, podemos aplicar SVM kernelizado simplemente cambiando nuestro kernel lineal a un kernel RBF (función de base radial), utilizando el hiperparámetro del modelo ``kernel``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dvOe56pJcBL"
      },
      "source": [
        "clf = SVC(kernel='rbf', C=1E6, gamma='auto')\n",
        "clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWaEUJjoJcBO"
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kDz9nCtJcBU"
      },
      "source": [
        "Utilizando esta máquina de vectores de soporte kernelizada, aprendemos un límite de decisión no lineal adecuado.\n",
        "Esta estrategia de transformación del núcleo se utiliza a menudo en el aprendizaje automático para convertir los métodos lineales rápidos en métodos no lineales rápidos, especialmente para los modelos en los que se puede utilizar el truco del núcleo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMegiEL_JcBe"
      },
      "source": [
        "## Ejemplo práctico: Reconocimiento de caras\n",
        "\n",
        "Como ejemplo de máquinas de vectores soporte en acción, veamos el problema del reconocimiento facial.\n",
        "Utilizaremos el conjunto de datos Labeled Faces in the Wild, que consiste en varios miles de fotos cotejadas de diversas figuras públicas.\n",
        "En Scikit-Learn se ha incorporado un recuperador para el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpGCtyTVJcBg"
      },
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPGVcKhPJcBk"
      },
      "source": [
        "Vamos a trazar algunas de estas caras para ver con qué estamos trabajando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhKloIGDJcBl"
      },
      "source": [
        "fig, ax = plt.subplots(3, 5)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[],\n",
        "            xlabel=faces.target_names[faces.target[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj27TYZVJcBo"
      },
      "source": [
        "Cada imagen contiene [62×47] o casi 3.000 píxeles.\n",
        "Podríamos proceder simplemente utilizando el valor de cada píxel como una característica, pero a menudo es más eficaz utilizar algún tipo de preprocesador para extraer características más significativas; aquí vamos a utilizar un análisis de componentes principales (véase [En profundidad: Análisis de componentes principales](05.09-Principal-Component-Analysis.ipynb)) para extraer 150 componentes fundamentales para alimentar nuestro clasificador de máquina de vectores de apoyo.\n",
        "\n",
        "Podemos hacer esto de la manera más directa empaquetando el preprocesador y el clasificador en una sola tubería:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmLUrOwLJcBp"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
        "svc = SVC(kernel='rbf', class_weight='balanced')\n",
        "model = make_pipeline(pca, svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKFxekmNJcBt"
      },
      "source": [
        "Para probar el resultado de nuestro clasificador, dividiremos los datos en un conjunto de entrenamiento y otro de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsa07DFnJcBu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
        "                                                random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkhjMeZdJcBx"
      },
      "source": [
        "Por último, podemos utilizar una validación cruzada de búsqueda en cuadrícula para explorar combinaciones de parámetros.\n",
        "Aquí ajustaremos ``C`` (que controla la dureza del margen) y ``gamma`` (que controla el tamaño del núcleo de la función de base radial), y determinaremos el mejor modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj6SmqgsJcBy"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'svc__C': [1, 5, 10, 50],\n",
        "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "%time grid.fit(Xtrain, ytrain)\n",
        "print(grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh-gbWJwJcB4"
      },
      "source": [
        "Los valores óptimos caen hacia el centro de nuestra cuadrícula; si cayeran en los bordes, querríamos ampliar la cuadrícula para asegurarnos de que hemos encontrado el verdadero óptimo.\n",
        "\n",
        "Ahora, con este modelo validado de forma cruzada, podemos predecir las etiquetas de los datos de prueba, que el modelo aún no ha visto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqm5OdpPJcB5"
      },
      "source": [
        "model = grid.best_estimator_\n",
        "yfit = model.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V91Za4HrJcB-"
      },
      "source": [
        "Veamos algunas de las imágenes de prueba junto con sus valores previstos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0tUu-2qJcCA"
      },
      "source": [
        "fig, ax = plt.subplots(4, 6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
        "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
        "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbfG7UX8JcCD"
      },
      "source": [
        "De esta pequeña muestra, nuestro estimador óptimo sólo etiquetó mal una cara (la de Bush de Bush, en la fila inferior, fue etiquetada erróneamente como Blair).\n",
        "\n",
        "Podemos hacernos una mejor idea del rendimiento de nuestro estimador utilizando el informe de clasificación, que enumera las estadísticas de recuperación etiqueta por etiqueta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmnDNsSzJcCE"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, yfit,\n",
        "                            target_names=faces.target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1t4TrjJcCH"
      },
      "source": [
        "También podríamos mostrar la matriz de confusión entre estas clases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je5YiLkIJcCI"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(ytest, yfit)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=faces.target_names,\n",
        "            yticklabels=faces.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeZ5Ib5dJcCM"
      },
      "source": [
        "Esto nos ayuda a tener una idea de qué etiquetas pueden ser confundidas por el estimador.\n",
        "\n",
        "Para una tarea de reconocimiento facial en el mundo real, en la que las fotos no vienen previamente recortadas en bonitas cuadrículas, la única diferencia en el esquema de clasificación facial es la selección de características: habría que utilizar un algoritmo más sofisticado para encontrar las caras, y extraer características que sean independientes de la pixelación.\n",
        "\n",
        "Para este tipo de aplicaciones, una buena opción es hacer uso de [OpenCV](http://opencv.org), que, entre otras cosas, incluye implementaciones preentrenadas de herramientas de extracción de características de última generación para imágenes en general y rostros en particular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crDt2C9VJcCN"
      },
      "source": [
        "## Resumen de las máquinas de vectores de soporte\n",
        "\n",
        "Hemos visto aquí una breve introducción intuitiva a los principios en los que se basan las máquinas de vectores de soporte.\n",
        "Estos métodos son un potente método de clasificación por varias razones:\n",
        "\n",
        "- Su dependencia de relativamente pocos vectores de soporte significa que son modelos muy compactos, y ocupan muy poca memoria.\n",
        "- Una vez entrenado el modelo, la fase de predicción es muy rápida.\n",
        "- Como sólo se ven afectados por los puntos cercanos al margen, funcionan bien con datos de alta dimensión, incluso con datos con más dimensiones que muestras, lo que constituye un régimen difícil para otros algoritmos.\n",
        "- Su integración con métodos kernel los hace muy versátiles, capaces de adaptarse a muchos tipos de datos.\n",
        "\n",
        "Sin embargo, las SVM también tienen varias desventajas:\n",
        "\n",
        "- El escalado con el número de muestras $N$ es $\\mathcal{O}[N^3]$ en el peor de los casos, o $\\mathcal{O}[N^2]$ para implementaciones eficientes. Para un gran número de muestras de entrenamiento, este coste computacional puede ser prohibitivo.\n",
        "- Los resultados dependen en gran medida de una elección adecuada del parámetro de suavización $C$. Este parámetro debe elegirse cuidadosamente mediante validación cruzada, lo que puede resultar caro a medida que los conjuntos de datos aumentan de tamaño.\n",
        "- Los resultados no tienen una interpretación probabilística directa. Puede estimarse mediante una validación cruzada interna (véase el parámetro ``probabilidad`` de ``SVC``), pero esta estimación adicional es costosa.\n",
        "\n",
        "Teniendo en cuenta estos rasgos, generalmente sólo recurro a las SVM cuando otros métodos más sencillos, más rápidos y con menos ajustes han demostrado ser insuficientes para mis necesidades.\n",
        "No obstante, si tiene los ciclos de CPU necesarios para entrenar y validar de forma cruzada una SVM en sus datos, el método puede dar excelentes resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UGJm5zfKD5k"
      },
      "source": [
        "# Sources:\n",
        "\n",
        "https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/\n",
        "\n",
        "https://medium.com/@kbrook10/day-11-machine-learning-using-knn-k-nearest-neighbors-with-scikit-learn-350c3a1402e6\n",
        "\n",
        "This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas"
      ]
    }
  ]
}